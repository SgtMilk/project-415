{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.util import random_noise\n",
    "import gc\n",
    "\n",
    "data_folder = os.path.abspath('./data')\n",
    "image_folder = os.path.join(data_folder, 'TRANCOS')\n",
    "\n",
    "print(image_folder)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available() == False:\n",
    "    print('\\033[91m' + \"You are training on CPU, are you sure you want to continue? We give you no choice tho.\" + '\\033[0m')\n",
    "\n",
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/felix/git/project-415/data/TRANCOS\n",
      "\u001b[91mYou are training on CPU, are you sure you want to continue? We give you no choice tho.\u001b[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fada4546f30>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Hyperparameters\n",
    "img_size = (224, 224, 3)\n",
    "batch_size = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# dataset class\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, type):\n",
    "\n",
    "        self.type = type\n",
    "        csv = pd.read_csv(os.path.join(data_folder, self.type + '.csv'))\n",
    "\n",
    "        if self.type == 'train' or self.type == 'valid':\n",
    "            csv = pd.Series(csv.counts.values,index=csv.images).to_dict()\n",
    "\n",
    "            # getting the images \n",
    "            self.images = []\n",
    "            self.labels = []\n",
    "            images = list(csv.keys())\n",
    "            labels = list(csv.values())\n",
    "            if len(images) != len(labels):\n",
    "                raise ValueError(\"Image and label arrays do not have the same size.\")\n",
    "\n",
    "            for img, label in zip(images, labels):\n",
    "\n",
    "                # resizing and normalizing base image\n",
    "                image = cv2.normalize(cv2.resize(plt.imread(os.path.join(image_folder, img)), img_size[:2]), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                self.images.append(torch.from_numpy(image).to(device))\n",
    "                self.labels.append(label)\n",
    "\n",
    "                # data augmentation\n",
    "                if self.type == 'train':\n",
    "                    self.images.append(torch.from_numpy(np.fliplr(image).copy()).to(device))\n",
    "                    self.images.append(torch.from_numpy(np.flipud(image).copy()).to(device))\n",
    "                    for i in range(2):\n",
    "                        self.labels.append(label)\n",
    "\n",
    "            if len(self.images) != len(self.labels):\n",
    "                raise ValueError(\"Image and label arrays do not have the same size or some values in these arrays are None.\")\n",
    "\n",
    "        elif self.type == 'test':\n",
    "            # getting the images\n",
    "            self.images = []\n",
    "            self.labels = []\n",
    "            images = list(csv.images.values.tolist())\n",
    "            labels = list(csv.images.values.tolist())\n",
    "            for img, label in zip(images, labels):\n",
    "                # resizing and normalizing base image\n",
    "                image = cv2.normalize(cv2.resize(plt.imread(os.path.join(image_folder, img)), img_size[:2]), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                self.images.append(torch.from_numpy(image).to(device))\n",
    "                self.labels.append(label)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid type in dataset. It has to have one of the following values: 'train', 'valid', 'test'.\")\n",
    "        print(\"Initiated \" + self.type + \" dataset of size \" + str(len(self.images)) + \" and images of shape \" + str(list(img_size)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.type == 'test':\n",
    "            return self.images[i], self.labels[i]\n",
    "        else:\n",
    "            return self.images[i], self.labels[i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# declaring datasets and data loaders\n",
    "\n",
    "train_dataset = CustomDataset('train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomDataset('valid')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset('test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initiated train dataset of size 1869 and images of shape [224, 224, 3]\n",
      "Initiated valid dataset of size 200 and images of shape [224, 224, 3]\n",
      "Initiated test dataset of size 421 and images of shape [224, 224, 3]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# declaring parameters\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 1\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# model\n",
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224 \n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch Version:  1.10.0+cpu\n",
      "Torchvision Version:  0.11.1+cpu\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# training\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "\n",
    "                    inputs = inputs.permute(0,3,1,2) # remap the inputs at the good spot\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs[:,0], labels.float()) # cast the output as a 1D tensor and the inputs as float\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val':valid_loader}\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders, loss, optimizer_ft, num_epochs=num_epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 161.8518 Acc: 0.0000\n",
      "val Loss: 95.3218 Acc: 0.0000\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 176.2216 Acc: 0.0000\n",
      "val Loss: 109.5917 Acc: 0.0000\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 150.5689 Acc: 0.0000\n",
      "val Loss: 83.0742 Acc: 0.0000\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 174.7922 Acc: 0.0000\n",
      "val Loss: 126.6620 Acc: 0.0000\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 161.2619 Acc: 0.0000\n",
      "val Loss: 77.3981 Acc: 0.0000\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 169.5452 Acc: 0.0000\n",
      "val Loss: 87.2940 Acc: 0.0000\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 157.6552 Acc: 0.0000\n",
      "val Loss: 79.6393 Acc: 0.0000\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 155.9930 Acc: 0.0000\n",
      "val Loss: 92.9386 Acc: 0.0000\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 145.2245 Acc: 0.0000\n",
      "val Loss: 83.7868 Acc: 0.0000\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 144.4588 Acc: 0.0000\n",
      "val Loss: 76.9648 Acc: 0.0000\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 160.9428 Acc: 0.0000\n",
      "val Loss: 122.6766 Acc: 0.0000\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 166.8580 Acc: 0.0000\n",
      "val Loss: 88.6203 Acc: 0.0000\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 146.8367 Acc: 0.0000\n",
      "val Loss: 82.8352 Acc: 0.0000\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 155.8078 Acc: 0.0000\n",
      "val Loss: 76.0741 Acc: 0.0000\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 153.0177 Acc: 0.0000\n",
      "val Loss: 88.2572 Acc: 0.0000\n",
      "\n",
      "Training complete in 34m 6s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# training results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# saving model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "# calculating accuracy of model\n",
    "import csv\n",
    "\n",
    "model_ft.eval()\n",
    "\n",
    "# im_name = []\n",
    "# count = []\n",
    "# for inputs, name in test_loader:\n",
    "#     inputs = inputs.permute(0,3,1,2) # remap the inputs at the good spot\n",
    "#     outputs = model_ft(inputs)\n",
    "#     out_int = [int(item) for item in outputs[:,0].tolist()]\n",
    "#     count.extend(out_int)\n",
    "#     im_name.extend(name)\n",
    "\n",
    "out = [['images'], ['counts']]\n",
    "out[0].extend(im_name)\n",
    "out[1].extend(count)\n",
    "\n",
    "out2 = zip(*out)\n",
    "out3 = list(out2)\n",
    "\n",
    "with open('test_out.csv', 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    for x in out3:\n",
    "        write.writerow(x)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}